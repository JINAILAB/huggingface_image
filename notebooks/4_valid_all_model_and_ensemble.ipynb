{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model validation 과 ensemble\n",
    "\n",
    "이 강의에서는 여러 모델을 validation하고, 각 모델의 예측을 ensemble하여 성능을 향상시키는 방법에 대해 다룹니다. 검증 데이터를 전처리하고, 개별 모델의 평가 지표를 계산한 후, 여러 모델의 예측을 결합하여 최종 앙상블 결과를 도출하는 과정까지 설명할 것입니다. 최종 목표는 동일한 데이터셋에 대해 각 모델을 검증하고, 이들의 예측을 모아서 성능을 높이는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 상위 폴더로 이동\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from data_presets import ClassificationPresetEval\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from datasets import load_from_disk\n",
    "from image_utils import CustomImagePipeline, run_model, get_metric_from_df\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config와 모델 경로 설정\n",
    "pretrain된 모델들이 모여있는 폴더와 dataset의 경로를 설정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = [\n",
    "    \"./cifar100_facebook_convnext-small-224\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config parameter 설정\n",
    "config = {\n",
    "    \"dataset_dir\" : '/workspace/model/hugginface_image_classifcation/data/hfdataset/test',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dirs = []\n",
    "for folder in folder_paths:\n",
    "    hf_dirs.extend(glob.glob(f\"{folder}/*/best.hf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비\n",
    "\n",
    "우리는 먼저 검증에 사용할 데이터셋을 로드합니다. 이 데이터셋은 Hugging Face의 `load_from_disk` 포맷으로 저장되어 있으며, 검증 데이터셋을 로드한 후 라벨을 가져옵니다. 또한, 데이터셋에 있는 라벨 이름과 ID 간의 매핑을 생성합니다. 이는 이후 모델 예측과 실제 라벨을 비교하는 데 필요합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(config['dataset_dir'])\n",
    "labels = dataset['test'].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_valid(batch, valid_transforms):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    if \"image\" in batch:\n",
    "        batch[\"pixel_values\"] = [\n",
    "            valid_transforms(image.convert(\"RGB\")) for image in batch[\"image\"]\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "valid_transform = ClassificationPresetEval(\n",
    "            crop_size=224,\n",
    "            resize_size=256,\n",
    "            interpolation=InterpolationMode.BILINEAR,\n",
    "            use_v2=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = dataset['test']\n",
    "valid_ds.set_transform(lambda batch: preprocess_valid(batch, valid_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 검증\n",
    "\n",
    "모델 검증 단계에서는 사전 학습된 모델을 로드하고, 각 모델에 대해 데이터를 입력하여 예측 결과를 도출합니다. 이를 통해 모델의 성능을 평가할 수 있는 여러 지표를 계산합니다. 이 강의에서는 각 모델의 정확도, 혼동 행렬 등의 평가 지표를 수집할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric(model_dir, dataset, id2label):\n",
    "    answer_df = run_model(model_dir, dataset, id2label)\n",
    "    metric_dict = get_metric_from_df(answer_df)\n",
    "    return metric_dict, answer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가 지표 수집\n",
    "\n",
    "모든 모델의 성능을 평가한 후, 각 모델의 평가 지표를 데이터프레임으로 변환하여 비교 분석할 수 있습니다. 이를 통해 모델 간의 성능 차이를 쉽게 파악할 수 있으며, 어떤 모델이 가장 우수한지 평가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "metrics_list = []\n",
    "answer_dfs = []\n",
    "for model_dir in hf_dirs:\n",
    "    metric_dict, answer_df = get_metric(model_dir, valid_ds, id2label)\n",
    "    metric_dict['model_dir'] = model_dir\n",
    "    metrics_list.append(metric_dict)\n",
    "    answer_dfs.append(answer_df)\n",
    "\n",
    "# Convert the collected metrics into a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>top2_accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>specificity</th>\n",
       "      <th>model_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.9314</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>0.998036</td>\n",
       "      <td>0.998408</td>\n",
       "      <td>./cifar100_facebook_convnext-small-224/convnex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>0.998401</td>\n",
       "      <td>0.998590</td>\n",
       "      <td>./cifar100_facebook_convnext-small-224/convnex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.998392</td>\n",
       "      <td>0.998611</td>\n",
       "      <td>./cifar100_facebook_convnext-small-224/convnex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   accuracy  top2_accuracy  precision  recall  f1_score   roc_auc  \\\n",
       "0    0.8424         0.9314     0.8424  0.8424    0.8424  0.998036   \n",
       "1    0.8604         0.9382     0.8604  0.8604    0.8604  0.998401   \n",
       "2    0.8625         0.9387     0.8625  0.8625    0.8625  0.998392   \n",
       "\n",
       "   specificity                                          model_dir  \n",
       "0     0.998408  ./cifar100_facebook_convnext-small-224/convnex...  \n",
       "1     0.998590  ./cifar100_facebook_convnext-small-224/convnex...  \n",
       "2     0.998611  ./cifar100_facebook_convnext-small-224/convnex...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in answer_dfs:\n",
    "    df['idx_dir'] = valid_ds['dirs']\n",
    "    df = df.drop('encoded_label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 방법\n",
    "\n",
    "앙상블은 여러 모델의 예측을 결합하여 최종 예측을 생성하는 방법입니다. 여기서는 두 가지 앙상블 방식을 사용합니다: **소프트 보팅**(soft voting)과 **하드 보팅**(hard voting). 소프트 보팅은 각 모델의 확률 예측 값을 평균 내어 최종 클래스를 선택하는 방식이고, 하드 보팅은 각 모델이 예측한 클래스 중 다수결을 통해 최종 클래스를 선택하는 방식입니다.\n",
    "\n",
    "- **소프트 보팅**: 모델들이 예측한 클래스 확률 값의 평균을 내고, 가장 높은 확률을 가진 클래스를 최종 예측으로 선택합니다. 이 방법은 모델들이 예측할 때 자신 없는 클래스를 잘 고려할 수 있다는 장점이 있습니다.\n",
    "- **하드 보팅**: 모델들이 예측한 클래스 중 가장 많이 나온 클래스를 최종 예측으로 선택합니다. 각 모델의 예측 결과를 다수결로 결정하는 방식입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import mode\n",
    "\n",
    "def ensemble_prediction_fixed(dfs, top_k=1, voting='soft'):\n",
    "    # 여러 모델에서 나온 softmax 예측값을 결합\n",
    "    \n",
    "    label_encoding = dfs[0].iloc[:, :-2].columns\n",
    "    # 실제 라벨 추출\n",
    "    true_labels = dfs[0]['label']\n",
    "    \n",
    "    # LabelEncoder 초기화 및 실제 라벨에 맞춰 학습\n",
    "    le = LabelEncoder()\n",
    "    le.fit(label_encoding)\n",
    "    \n",
    "    # 실제 라벨을 인코딩\n",
    "    encoded_true_labels = le.transform(true_labels)\n",
    "    \n",
    "    # 예측 DataFrame에서 LabelEncoder 클래스 순서에 맞춰 열 정렬\n",
    "    prob_columns = le.classes_\n",
    "    prob_preds = []\n",
    "    for file in dfs:\n",
    "        # 'label' 및 'idx_dir' 열 삭제\n",
    "        prob_df = file.drop(columns=['label', 'idx_dir'])\n",
    "        # 열을 정렬\n",
    "        prob_df = prob_df[prob_columns]\n",
    "        prob_preds.append(prob_df)\n",
    "    \n",
    "    # prob_preds는 이제 le.classes_ 순서로 정렬된 DataFrame 리스트\n",
    "    if voting == 'soft':\n",
    "        # 소프트 보팅: 확률 평균\n",
    "        ensemble_softmax = np.mean([pred.values for pred in prob_preds], axis=0)\n",
    "        \n",
    "        # ensemble_softmax에서 예측된 클래스 인덱스 가져오기\n",
    "        ensemble_predictions_indices = np.argmax(ensemble_softmax, axis=1)\n",
    "        \n",
    "        # top-k 정확도 수동 계산\n",
    "        # top-k 예측 인덱스 가져오기\n",
    "        topk_preds_indices = np.argsort(-ensemble_softmax, axis=1)[:, :top_k]\n",
    "        # 실제 라벨이 top-k 예측 중 하나인지 확인\n",
    "        topk_correct = [encoded_true_labels[i] in topk_preds_indices[i] for i in range(len(encoded_true_labels))]\n",
    "        acc_topk = np.mean(topk_correct)\n",
    "    \n",
    "    elif voting == 'hard':\n",
    "        # 하드 보팅: 각 모델의 예측 클래스 인덱스 가져오기\n",
    "        predictions_indices = [np.argmax(pred.values, axis=1) for pred in prob_preds]\n",
    "        \n",
    "        # 예측 인덱스를 (n_samples, n_models) 형태로 변환\n",
    "        predictions_indices = np.array(predictions_indices).T  # (n_samples, n_models)\n",
    "        \n",
    "        # 각 샘플에 대해 최빈값 계산\n",
    "        ensemble_predictions_indices, _ = mode(predictions_indices, axis=1)\n",
    "        \n",
    "        # 결과를 1차원 배열로 변환\n",
    "        ensemble_predictions_indices = ensemble_predictions_indices.flatten()\n",
    "        \n",
    "        # top-k 정확도는 하드 보팅에서는 의미가 없어 생략하거나 다른 방식으로 처리 가능\n",
    "        acc_topk = None  # 하드 보팅에서는 top-k 정확도 의미가 없음\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"voting은 'soft' 또는 'hard'이어야 합니다.\")\n",
    "    \n",
    "    # 예측된 인덱스를 원래 클래스 라벨로 변환\n",
    "    ensemble_predictions = le.inverse_transform(ensemble_predictions_indices)\n",
    "    \n",
    "    # 정확도 계산\n",
    "    acc = accuracy_score(encoded_true_labels, ensemble_predictions_indices)\n",
    "    \n",
    "    # 혼동 행렬 계산\n",
    "    conf_matrix = confusion_matrix(encoded_true_labels, ensemble_predictions_indices)\n",
    "    \n",
    "    # 앙상블 데이터프레임 생성 (예측 클래스 포함)\n",
    "    ensemble_df = pd.DataFrame({\n",
    "        'ensemble_prediction': ensemble_predictions,\n",
    "        'true_label': true_labels,\n",
    "        'idx_dir': dfs[0]['idx_dir']\n",
    "    })\n",
    "    \n",
    "    return acc, acc_topk, conf_matrix, ensemble_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 성능 평가\n",
    "\n",
    "앙상블을 통해 얻은 예측 결과에 대해 정확도와 혼동 행렬을 계산합니다. 이를 통해 앙상블의 성능이 개별 모델보다 얼마나 향상되었는지 확인할 수 있습니다. 또한, Top-k 정확도도 함께 계산하여, 실제 라벨이 상위 k개의 예측 중 하나에 포함되는지를 평가할 수 있습니다.\n",
    "\n",
    "이 과정에서 우리는 소프트 보팅과 하드 보팅 방식을 비교하여, 각 방식이 어떤 상황에서 더 적합한지 설명하고, 최종적으로 더 나은 성능을 제공하는 앙상블 방식을 선택할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function with soft voting and top_k=3 as an example\n",
    "acc, acc_topk, conf_matrix, ensemble_df = ensemble_prediction_fixed(answer_dfs, top_k=3, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8806, 0.9707)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, acc_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, acc_topk, conf_matrix, ensemble_df = ensemble_prediction_fixed(answer_dfs, top_k=3, voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8743, None)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, acc_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensemble_prediction</th>\n",
       "      <th>true_label</th>\n",
       "      <th>idx_dir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mountain</td>\n",
       "      <td>mountain</td>\n",
       "      <td>./data/cifar100_images/test/mountain/image_0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mountain</td>\n",
       "      <td>mountain</td>\n",
       "      <td>./data/cifar100_images/test/mountain/image_21.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mountain</td>\n",
       "      <td>mountain</td>\n",
       "      <td>./data/cifar100_images/test/mountain/image_42.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mountain</td>\n",
       "      <td>mountain</td>\n",
       "      <td>./data/cifar100_images/test/mountain/image_212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mountain</td>\n",
       "      <td>mountain</td>\n",
       "      <td>./data/cifar100_images/test/mountain/image_397...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>./data/cifar100_images/test/tiger/image_8975.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>./data/cifar100_images/test/tiger/image_9002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>./data/cifar100_images/test/tiger/image_9079.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>./data/cifar100_images/test/tiger/image_9490.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>./data/cifar100_images/test/tiger/image_9525.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ensemble_prediction true_label  \\\n",
       "0               mountain   mountain   \n",
       "1               mountain   mountain   \n",
       "2               mountain   mountain   \n",
       "3               mountain   mountain   \n",
       "4               mountain   mountain   \n",
       "...                  ...        ...   \n",
       "9995               tiger      tiger   \n",
       "9996               tiger      tiger   \n",
       "9997               tiger      tiger   \n",
       "9998               tiger      tiger   \n",
       "9999               tiger      tiger   \n",
       "\n",
       "                                                idx_dir  \n",
       "0      ./data/cifar100_images/test/mountain/image_0.png  \n",
       "1     ./data/cifar100_images/test/mountain/image_21.png  \n",
       "2     ./data/cifar100_images/test/mountain/image_42.png  \n",
       "3     ./data/cifar100_images/test/mountain/image_212...  \n",
       "4     ./data/cifar100_images/test/mountain/image_397...  \n",
       "...                                                 ...  \n",
       "9995   ./data/cifar100_images/test/tiger/image_8975.png  \n",
       "9996   ./data/cifar100_images/test/tiger/image_9002.png  \n",
       "9997   ./data/cifar100_images/test/tiger/image_9079.png  \n",
       "9998   ./data/cifar100_images/test/tiger/image_9490.png  \n",
       "9999   ./data/cifar100_images/test/tiger/image_9525.png  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
